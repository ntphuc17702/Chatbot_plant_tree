
import sys, io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
import os, json, uuid
import numpy as np
import faiss
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from extractors import auto_extract
from utils import extract_summary, extract_keywords
from config_loader import load_config

config = load_config()


# ---------------------------------------------------------------
# üîß C·∫•u h√¨nh

BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # th∆∞ m·ª•c ch·ª©a file hi·ªán t·∫°i
INDEX_FILE = os.path.join(BASE_DIR, "..", "faiss.index")
META_FILE = os.path.join(BASE_DIR, "..", "docs.json")
DATA_DIR = os.path.join(BASE_DIR, "..", "data_output")

MODEL_NAME = config["model"]["embedding_model"]
embedder = SentenceTransformer(MODEL_NAME)
dimension = embedder.get_sentence_embedding_dimension()

# ---------------------------------------------------------------
# üß† T·∫°o ho·∫∑c load FAISS index
if os.path.exists(INDEX_FILE):
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "r", encoding="utf-8") as f:
        docs = json.load(f)
else:
    index = faiss.IndexFlatIP(dimension)  # cosine similarity
    docs = []

# ---------------------------------------------------------------
# üìÑ H√†m x·ª≠ l√Ω 1 file ƒë∆°n
def ingest_file(path):
    raw_text = auto_extract(path)
    if not path.lower().endswith(".docx"):
        print(f"[!] B·ªè qua (kh√¥ng ph·∫£i docx): {path}")
        return


    if not raw_text.strip():
        print(f"[!] B·ªè qua file r·ªóng: {path}")
        return

    # T·∫°o 2 bi·ªÉu di·ªÖn: summary + keywords
    summary_text = extract_summary(raw_text)
    keyword_text = extract_keywords(raw_text)

    representations = {
        "summary": summary_text,
        "keywords": keyword_text
    }

    metas, vecs = [], []

    # Duy·ªát qua 2 bi·ªÉu di·ªÖn
    for rep_type, rep_text in representations.items():
        doc_id = str(uuid.uuid4())
        vec = embedder.encode(rep_text, normalize_embeddings=True)

        meta = {
            "id": doc_id,
            "source": os.path.basename(path),
            "rep_type": rep_type,   # summary / keywords
            "text": raw_text        # lu√¥n l∆∞u raw text
        }

        metas.append(meta)
        vecs.append(vec)

    # Th√™m 2 vector v√†o FAISS
    vecs_np = np.vstack(vecs).astype("float32")
    index.add(vecs_np)
    docs.extend(metas)

    # L∆∞u l·∫°i
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)

# ---------------------------------------------------------------
# üöÄ Ingest to√†n b·ªô th∆∞ m·ª•c
def ingest_folder(folder=DATA_DIR):
    # files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]
    files = [
        os.path.join(folder, f)
        for f in os.listdir(folder)
        if os.path.isfile(os.path.join(folder, f)) and f.lower().endswith(".docx")
    ]
    if not files:
        print("‚ö†Ô∏è Kh√¥ng c√≥ file n√†o trong th∆∞ m·ª•c c·∫ßn ingest.")
        return

    print(f"üìÅ ƒêang ingest {len(files)} file trong th∆∞ m·ª•c: {folder}\n")

    for path in tqdm(files, desc="üîÑ ƒêang x·ª≠ l√Ω", unit="file", ncols=90):
        ingest_file(path)

# ---------------------------------------------------------------
if __name__ == "__main__":
    print("üöÄ Multi-representation (summary + keywords) Embedding Started...\n")
    ingest_folder(DATA_DIR)
    print("\n‚úÖ Ho√†n t·∫•t embedding t·∫•t c·∫£ file.")
